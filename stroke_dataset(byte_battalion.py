# -*- coding: utf-8 -*-
"""STROKE_DATASET(BYTE_BATTALION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v166_yHNw9gVmn0hDMvv24nqYprXWlZj

###Import Libraries.........!!!!!!
"""

import warnings
warnings.filterwarnings("ignore")
#'pandas' is used for mathematical operations on large,mutli-dimensional arrays anmd matrices
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
#classidfication model
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn import metrics

"""###EXPLORATION+CLEANING."""

import pandas as pd
df= pd.read_csv("/content/stroke.csv")
df

df.info()

df.head()

df.tail()

df.columns

df.describe()

col = ['gender','age','hypertension','heart_disease','ever_married',
       'work_type','Residence_type','avg_glucose_level','bmi',
       'smoking_status','stroke']
for i in col:
  print(df[i].value_counts())

df.isnull().sum()

#missing value fiil_out with mean
df['bmi'].fillna(df['bmi'].mean(), inplace=True)

df.duplicated().sum()

df

df['stroke'].unique()

"""###EDA"""

df['stroke'].value_counts()

df.stroke.value_counts(normalize=True)

sns.countplot(x='stroke', data=df)
plt.title(" stroke Distribution")

plt.figure(figsize=(6,4))
sns.countplot(x="stroke", data=df)
plt.title("Countplot of stroke variable")
plt.xlabel("stroke")
plt.ylabel("Count")
plt.show()

import seaborn as sns

sns.pairplot(df, hue = "stroke")

#scatter plot -> two column Relationship
sns.scatterplot(data=df, x='age', y='bmi', hue='stroke', palette='coolwarm')
plt.title("Relationship between age,bmi and stroke")
plt.show()

sns.scatterplot(data=df, x='age', y='avg_glucose_level', hue='stroke', palette='coolwarm')
plt.title("Relationship between age,avg_glucose_level and stroke")
plt.show()

df.select_dtypes(include=['number']).corr()

plt.figure(figsize=(10,10))
correlation_matrix =df.select_dtypes(include=['number']).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of stroke dataset')
plt.show()

df.dtypes[df.dtypes=='object']

"""###post processing"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['gender']=le.fit_transform(df['gender'])
df['gender']

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['ever_married']=le.fit_transform(df['ever_married'])
df['ever_married']

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['work_type']=le.fit_transform(df['work_type'])
df['work_type']

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['Residence_type']=le.fit_transform(df['Residence_type'])
df['Residence_type']

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['smoking_status']=le.fit_transform(df['smoking_status'])
df['smoking_status']

Y = df['stroke']
X = df.drop(columns=['stroke', 'id'])

Y

X

"""###over_sampling"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_re, y_re = sm.fit_resample(X_train, Y_train)
X_train, X_test, Y_train, Y_test = train_test_split(X_re, y_re, test_size = 0.25)

sns.countplot(x ='stroke', data = pd.DataFrame({'stroke': Y}))
plt.title('Before applying SMOTE')
plt.show()

sns.countplot(x ='stroke', data = pd.DataFrame({'stroke': y_re}))
plt.title('After applying SMOTE')
plt.show()

print("Before SMOTE:")
print(Y.value_counts())

print("\nAfter SMOTE:")
print(y_re.value_counts())

X_train.shape, X_test.shape, Y_train.shape, Y_test.shape

"""###Modeling

RANDOM_FOREST_CLASSIFIER
"""

clf=RandomForestClassifier()
clf.fit(X_train,Y_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

classifier= RandomForestClassifier(n_estimators= 10, criterion="entropy")
classifier.fit(X_train,Y_train)

y_pred = classifier.predict(X_test)
score = accuracy_score(Y_test, y_pred)
accuracy = score*100
print(accuracy)

from sklearn.metrics import confusion_matrix
y_pred = clf.predict(X_test)

# PRINT THE CONFUSION MATRIX
print("Confusion Matrix")
cm = confusion_matrix(Y_test, y_pred)
print(cm)

plt.figure(figsize = (6, 4))
sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues', cbar = False, annot_kws = {'size' : 14})
plt.xlabel('Predicted Labels', fontsize = 14)
plt.ylabel('True Labels', fontsize = 14)
plt.title('Confusion Matrix', fontsize = 16)
plt.show()

from sklearn.metrics import classification_report
#generate classification report
report=classification_report(Y_test,y_pred)
print(report)

"""###PROJECT EXPLANATION
In this project, the dataset was first explored to understand its structure, including the number of rows(5109) and columns(12), data types are int(4) are float(3) and object(5).numeric Missing data was handled using numerical features were filled with the mean.In EDA process TARGET COLUMN IS "STROKE"we observed that features such as age, hypertension avg_glucose level are strongly core_related with (stroke),and also draw scatterplot,count_plot,pair_plot and correlation heatmap.(LE)Categorical variables were converted into numerical format using label encoding.The cleaned data was then split into independent variables (X) and the target variable (y).since the data is highly imbalanced techniques like SMOTE was used to baalnced the number of stroke and non stroke,using train_test split from scikit_learn.For modeling various machine learning models were trained,like (LOGISTIC REGRESSION accuracy is 82%), (DECISION TREEE CLASSIFIER accuracy is 91 ).(GNB accuracy is 81).(RANDOM FOREST CLASSIFIER accuracy is 93).(SVC acuraccy is 91).


** The perfect model training high accuracy is 93 (RANDOM_FOREST_CLASSIFIER) **
I used the Random Forest Classifier because it combines multiple decision trees to improve model accuracy and reduce overfitting.
It handles both numerical and categorical data efficiently and gives better generalization on unseen data.
Thats why it achieved a high accuracy of around 93%..
"""

